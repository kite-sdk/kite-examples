# Copyright 2013 Cloudera Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Application configuration file in HOCON format (Human-Optimized Config Object Notation). 
# HOCON syntax is defined at http://github.com/typesafehub/config/blob/master/HOCON.md
# and also used by Akka (http://www.akka.io) and Play (http://www.playframework.org/).
# For more examples see http://doc.akka.io/docs/akka/2.1.2/general/configuration.html

# morphline.conf example file
# this is a comment
// this is yet another comment

morphlines : [
  {
    id : morphline1
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]
    
    commands : [
      { separateAttachments {} }
      
      { 
        # auto-detect MIME type if it isn't explicitly supplied
        detectMimeType { 
          mimeTypesFiles : [mimetypes.xml,custom-mimetypes.xml]
        }
      }
      
      {
        tryRules {
          throwExceptionIfAllRulesFailed : true
          rules : [
            {
              commands : [ 
                { 
                  readCSV {
                    supportedMimeTypes : [text/csv]
                    separator : ","
                    columns : [first_name,"",last_name]
                    ignoreFirstLine : true
                    trim : true
                    charset : UTF-8
                  }
                }
              ]
            }      
      
            # next top-level rule:      
            {
              commands : [ 
                { 
                  "org.apache.solr.tika.TikaParser" { # captures metadata only
                    parsers : [ # nested Tika parsers
                      "org.apache.tika.parser.image.ImageParser"
                    ] 
                  }
                }
              ]           
            }      
      
            # next top-level rule:      
            {
              commands : [ 
                { decompress {} }           
                { callParentPipe {} } 
              ]
            }      
      
            # next top-level rule:      
            {
              commands : [ 
                { unpack {} }           
                { callParentPipe {} } 
              ]
            }      
      
            # next top-level rule:      
            {
              commands : [ 
                {
                  # wrap SolrCell around an HTML Tika parser
                  solrCell {
                    captureAttr : true # default is false
                    capture : [a, h1, h2] # extract some fields
                    fmap : { a : anchor, h1 : heading1 } # rename some fields
                    dateFormats : [ "yyyy-MM-dd'T'HH:mm:ss", "yyyy-MM-dd"] # various java.text.SimpleDateFormat
                    xpath : "/xhtml:html/xhtml:body/xhtml:div/descendant:node()"
                    uprefix : "ignored_"
                    lowernames : true
                    solrContentHandlerFactory : org.apache.solr.tika.TrimSolrContentHandlerFactory
                    
                    # Tika parsers to be registered. If multiple parsers support the same MIME type, 
                    # the parser is chosen that is closest to the bottom in this list:
                    parsers : [                    
                      { parser : org.apache.tika.parser.asm.ClassParser }
                      # { parser : org.gagravarr.tika.OggParser, additionalSupportedMimeTypes : [audio/ogg] }
                      { parser : org.gagravarr.tika.FlacParser }
                      { parser : org.apache.tika.parser.audio.AudioParser }                      
                      { parser : org.apache.tika.parser.audio.MidiParser }
                      { parser : org.apache.tika.parser.crypto.Pkcs7Parser }
                      { parser : org.apache.tika.parser.dwg.DWGParser }
                      { parser : org.apache.tika.parser.epub.EpubParser }
                      { parser : org.apache.tika.parser.executable.ExecutableParser }
                      { parser : org.apache.tika.parser.feed.FeedParser }
                      { parser : org.apache.tika.parser.font.AdobeFontMetricParser }
                      { parser : org.apache.tika.parser.font.TrueTypeParser }
                      { parser : org.apache.tika.parser.xml.XMLParser }                      
                      { parser : org.apache.tika.parser.html.HtmlParser }
                      { parser : org.apache.tika.parser.image.ImageParser }
                      { parser : org.apache.tika.parser.image.PSDParser }
                      { parser : org.apache.tika.parser.image.TiffParser }
                      { parser : org.apache.tika.parser.iptc.IptcAnpaParser }
                      { parser : org.apache.tika.parser.iwork.IWorkPackageParser }
                      { parser : org.apache.tika.parser.jpeg.JpegParser }
                      { parser : org.apache.tika.parser.mail.RFC822Parser }
                      { parser : org.apache.tika.parser.mbox.MboxParser, additionalSupportedMimeTypes : [message/x-emlx] }
                      { parser : org.apache.tika.parser.microsoft.OfficeParser }
                      { parser : org.apache.tika.parser.microsoft.TNEFParser }
                      { parser : org.apache.tika.parser.microsoft.ooxml.OOXMLParser }
                      { parser : org.apache.tika.parser.mp3.Mp3Parser }
                      { parser : org.apache.tika.parser.mp4.MP4Parser }
                      { parser : org.apache.tika.parser.hdf.HDFParser }
                      { parser : org.apache.tika.parser.netcdf.NetCDFParser }
                      { parser : org.apache.tika.parser.odf.OpenDocumentParser }
                      { parser : org.apache.tika.parser.pdf.PDFParser }
                      { parser : org.apache.tika.parser.pkg.CompressorParser }
                      { parser : org.apache.tika.parser.pkg.PackageParser }
                      { parser : org.apache.tika.parser.rtf.RTFParser }
                      { parser : org.apache.tika.parser.txt.TXTParser }
                      { parser : org.apache.tika.parser.video.FLVParser }
                      { parser : org.apache.tika.parser.xml.DcXMLParser }
                      { parser : org.apache.tika.parser.xml.FictionBookParser }
                      { parser : org.apache.tika.parser.chm.ChmParser }
                          
#                      org.apache.solr.tika.AvroTestParser
#                      org.apache.solr.tika.TwitterTestParser 
#                      org.apache.solr.tika.parser.NullParser 
#                      org.apache.solr.tika.parser.SSVParser 
#                      org.apache.solr.tika.parser.TSVParser 
#                      org.apache.solr.tika.parser.CSVParser 
#                      org.apache.solr.tika.parser.AvroContainerParser                      
                    ] 
                  }
                }
              ]
            }      
      
            # next top-level rule:      
            {
              # This top-level rule config can only accept events with headers that satisfy the following conditions:
              commands : [ 
                # predicate can be = (literal match) or != (not literal match) 
                # or =~ (regex match) or !~ (not regex match) 
                # or "java" (one or more Java statements where the last statement returns a boolean)
                { "=" { header_name1 : myAvroSchema } }
                { "=" { header_name2 : myAvroSource } }
                { java { code : """List tags = record.get("tags"); return tags.contains("hello");""" } }
                {
                  readAvroContainer {
                    supportedMimeTypes : [avro/binary]              
                    # readerSchemaString : "<json can go here>" # optional, avro json schema blurb for getSchema()
                    # readerSchemaFile : /path/to/syslog.avsc
                    addValues { # extract some values from hierarchical avro and add them to some fields
                      foo : { xquery : "/record/foo/path[1]" }
                      bar : { xquery : "/bookstore/book[price > 35]/price" }
                      baz : { xquery : "string(/record//stacktrace)" }
                    }       
                  }
                }
                { 
                  extractAvroPaths {
                    flatten : true
                    paths : {
                      field1 : "/record/foo/bar"
                      field2 : "/record/baz"                    
                    }
                  }
                }
                { extractFullAvroTree {} }
                { java 
                  { code : """
                      GenericData.Record avro = (GenericData.Record) record.getFirstValue("_attachment_body");
                      record.get("stacktraces").addAll(avro.get("stackTraces")); 
                      return child.process(record);
                           """ 
                  } 
                }                
              ]
            }      
      
            # next top-level rule:
            {        
              commands : [ 
                # Multiline log parser that supports "pattern", "what" and "negate" config params similar to logstash
                # For example, this can be used to parse log4j with stack traces
                # also see https://gist.github.com/smougenot/3182192 and http://logstash.net/docs/1.1.9/filters/multiline
                # also see http://logstash.net/docs/1.1.10/index
                # also see http://cookbook.logstash.net/recipes/syslog-pri/
                # also see http://cookbook.logstash.net/recipes/central-syslog/
                {
                  "org.apache.solr.tika.parser.LogParser" {
                    supportedMimeTypes : [text/log]
                    charset: UTF-8
                    recordSeparator: PATTERN # EOL|EOS|PATTERN
                    pattern : "(^.+Exception: .+)|(^\\s+at .+)|(^\\s+... \\d+ more)|(^\\s*Caused by:.+)"
                    negate: false
                    what : previous
                  }
                }
                
                {
                  addValues {
                    source_type : [text/log, text/log2]
                    source_host : myhost
                  }
                }
                 
                {                     
                  if { 
                    conditions : [
                      { logInfo { format : "evaluating conditions..." } }
          #            { fail {} }
                    ]
                    then : [
                      { logInfo { format : "processing then..." } }
                    ]
                    else : [
                      { logInfo { format : "processing else..." } }
                    ]
                  }
                }
                
                # next command tries some rules with conditions:
                { tryRules 
                  { 
                    throwExceptionIfAllRulesFailed : false
                    ignoreFoundNoMatchingRuleWarningsRegex : ".*" # default is ""
                    #ignorepreconditions : [] # FIXME
                    rules : [
                      {
                        commands : [
                          # if the conditions matches the regex groks are run and named capturing groups 
                          # extracted into those named record fields
                          # TODO: also support logstash grok patterns '%{SYNTAX:SEMANTIC}'
                          # predicate can be = (literal match) or != (not literal match) 
                          # or =~ (regex match) or !~ (not regex match) 
                          # or "java" (one or more Java statements where the last statement returns a boolean)
                          { "=~" { _raw : "(?<queue_field>\\d\\d\\d\\d-\\d\\d-\\)" } } 
                          { "=~" { _name_field : "(?<foo_field>\\d\\d\\d\\d)-(?<bar_field>\\d\\d\\d)" }
                            numMatches : atLeastOnce # all|atLeastOnce|once  
                            #if _name_field is multivalued field at least one value must match, default: atLeastOnce
                            extract : false # defaults to true
                          }
                          
                          { if 
                            { 
                              conditions : [ 
                                { grok { message : "(?<queue_field>\\d\\d\\d\\d-\\d\\d-\\)" } } 
                                { not { grok { message : "(?<queue_field>\\d\\d\\d\\d-\\d\\d-\\)" } } }
                              ]
                              then : [ { dropRecord {} } ]
                              else : []
                            }
                          }
                          
                          { java { code: """List tags = record.get("tags"); return tags.contains("hello") ? child.process(record) : false;""" } }

                          { addValues { timestamp : "@{last_modified}-@{year_field}" } }
                          { setValues { foo : "bar" } } 
                          { convertTimestamp {
                              field : ts1
                              inputFormats : ["yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", 
                                              "yyyy-MM-dd'T'HH:mm:ss", 
                                              "yyyy-MM-dd"]
                              inputTimezone : UTC
                              outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSSZ"                                 
                              outputTimezone : America/Los_Angeles
                            }
                          }
                          { grok { _raw : "(?<queue_field>\\d\\d\\d\\d-\\d\\d-\\)" } }
                          { replaceValues { find : "foo(.*)bar", replace : "$1", fields : [ timestamp, "meta_.*" ] } }
                          { replaceValues { find : "foo", predicate : "=", replace : "bar", fields : [ timestamp ] } }
                          { retainValues { find : "foo.*bar", fields : [ timestamp, _metadata_x, _metadata_y ] } }
                          { removeValues { find : "foo.*bar", fields : [ timestamp, _metadata_x, _metadata_y ] } }
                          { splitValues { separator : "\\s*,\\s*", fields : [ tags ] } }
                          { renameFields { firstName : first_name, lastModified : last_modified } }
                          { copyFields { firstName : first_name, lastModified : last_modified } }
                          { dropRecord { find : "foo.*bar", fields : [ timestamp, _metadata_x, _metadata_y ] } }
                          { dropRecord {} } # remove current record from morphline 
                          { "=~" { _name_field : "(?<foo_field>\\d\\d\\d\\d)-(?<bar_field>\\d\\d\\d)" }
                            numMatches : atLeastOnce # all|atLeastOnce|once
                            #if _name_field is multivalued field at least one value must match, default: atLeastOnce
                          }
                          { tryRules { 
                              rules : [
                                {
                                  commands: [] # TODO: something could go here
                                }
                              ]
                            }
                          }
                        ]
                      }
                    ]              
                  }
                }
      
                # next command is an action:
                # no need to send these fields to solr, do something like solrcell uprefix
                { removeFields { fields : [ "_metadata_.*", "aux_.*", "ignored_.*" ] } } 
              ]
            }      
          ]
        }
      }
    ]
            
    productionMode : false
    ignoreRecoverableExceptions : false
    ignoreLoads : false
  }
]

# Some examples for using variables:
MY_NAME_FIELD : my_first_name
MY_NAME_FIELDS : ${MY_NAME_FIELD}s
